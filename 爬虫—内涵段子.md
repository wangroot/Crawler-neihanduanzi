##爬虫———内涵段子

----------
<font size=4>python的基础学完，可以开始接触一下相关的项目的，虽然对一些库还不熟悉，但是我相信多练习，慢慢就会好起来的</br>
今天看到网上的内涵段子，自己也就试试去做一个爬一下内涵段子的内容</br>
准备工作：谷歌/火狐   pycharm  
步骤：
1.访问内涵段子!![](C:\Users\Administrator\Desktop\新建文件夹\1.png)
这是一个动态加载的网站</br>
2.抓包工具：抓包工具是拦截查看网络数据包内容的软件。</br>
最常见的抓包工具：[http://www.mottoin.com/97314.html](http://www.mottoin.com/97314.html)<留着自己学习！！！嘻嘻>但是这次用的是谷歌浏览器自带的抓包工具来进行的，谷歌浏览器上用F12，出现选项[](C:\Users\Administrator\Desktop\新建文件夹\2.png)</br>
3.这次爬虫用的是requests库————快速学习[http://docs.python-requests.org/zh_CN/latest/user/quickstart.html](http://docs.python-requests.org/zh_CN/latest/user/quickstart.html)
4.遇到的问题：网站的反爬措施；这次用的是Headers  [http://www.bubuko.com/infodetail-1942121.html](http://www.bubuko.com/infodetail-1942121.html)
这些可以解决这次的反爬措施</br>
5.接下来就是代码了</br>
import requests
import time
url1 = 'http://neihanshequ.com/joke/?is_json=1&app_name=neihanshequ_web&max_time=1521075470.0'
#获取段子信息  Json文件
html = requests.get(url1)
print(html)
print (html.text)
timestamp = html.json()['data']['max_time']
header = {'Accept':'application/json, text/javascript, */*; q=0.01',
        'Accept-Encoding':'gzip, deflate',
        'Accept-Language':'zh-CN,zh;q=0.8',
        'Connection':'keep-alive',
        'Cookie':'uuid="w:1cc888778bf94e5683cdd81fc877be7e"; tt_webid=6532783295012013576; csrftoken=d18962da5ac606fd9608d792273b1e9e; _ga=GA1.2.177855175.1521032137; _gid=GA1.2.1824019791.1521032137; _gat=1',
        'Host':'neihanshequ.com',
        'Referer':'http://neihanshequ.com/',
        'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.101 Safari/537.36',
        'X-CSRFToken':'d18962da5ac606fd9608d792273b1e9e',
        'X-Requested-With':'XMLHttpRequest'}
while type(timestamp)==float or type(timestamp)==int:
    time.sleep(3)
    url1 = 'http://neihanshequ.com/joke/?is_json=1&app_name=neihanshequ_web&max_time='+str(timestamp)
    with open('C:\\Users\\Administrator\\Desktop\\内涵段子.txt','a',encoding = 'utf-8') as f:
        html = requests.get(url1, headers=header)
        for n in range(len(html.json()['data']['data'])):
           data = html.json()['data']['data'][n]['group']['content']
           f.write(data +'\n')
    timestamp = html.json()['data']['max_time']
    print(timestamp)
#写入本地
